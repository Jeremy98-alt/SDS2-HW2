---
title: "Sapienza Jeremy 1960498 - HW2"
author: "Sapienza Jeremy 1960498"
date: "6/15/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache = TRUE)
```

## 1.a) Describing the Dugong's data

$$~$$

As the first step is important to illustrate the characteristics of the statistical model for dealing with the Dugong's data. Now, we load the main data file where within there are the data interested for our analysis.

```{r}
dugongdat <- read.delim2("./dugong-data.txt", header = TRUE, sep = "", dec = ",", stringsAsFactors = FALSE)
dugongdat <- dugongdat[, -c(1)]
head(dugongdat)
```

$$~$$

Now, we save the column of lenghts within $y_{obs}$ and the column of ages within $x_{i}$. After that we can show some characteristics of these two variables:

```{r echo=FALSE, include=FALSE}
y_obs <- as.numeric(dugongdat$Length)
x_i <- as.numeric(dugongdat$Age)
```

### **Information about the lengths of the dugongs**

$$~$$

```{r fig.align="center"}
cat(paste("The variance of the observations is: ", round(var(y_obs), 3), "\n", sep =""))
cat("Meanwhile the other features are...\n")
summary(y_obs)
hist(y_obs, col="orchid2", main="Distribution of the lenght y_obs", xlab="Length")
rug(y_obs, lwd=2)
```

Instead, the...

### **Information about the ages of the dugongs**

$$~$$

```{r fig.align="center"}
cat(paste("The variance of the observations is: ", round(var(x_i), 3), "\n", sep =""))
cat("Meanwhile the other features are...\n")
summary(x_i)
hist(x_i, col="orchid2", main="Distribution of the age x_i", xlab="Age")
rug(x_i, lwd=2)
```

### **Analyzing the informations together**

$$~$$

As we can seen, in this homework the statistical model is described by a non-linear regression model; for this reason we want to see something interest considering these two variables together:

```{r echo=FALSE, fig.align="center"}
plot(x_i, y_obs, col="orchid2", xlab="Age", ylab="Length", pch = 19, main = "The Dugong's Data")
grid()
```

As you can seen above, this suggests that the statistical model should be a *non-linear* regression, also the fact of the correlation of these two variables is $\sim \textit{0.8296}$ and the covariance is $\sim \textit{1.7932}$.

$$~$$

# 1.b) Deriving the likelihood function

$$~$$

In order to establish the likelihood function, we assume that the $Y_{i}$ are *i.i.d*. We define the likelihood function in this way, cosidering that $Y_{i} \sim N(\mu_{i}, \tau^{2})$:

$$
L_{y_{obs}}(\alpha, \beta, \gamma, x_{i}, \tau^{2}) = \prod_{i=1}^{n} f(y_i|\alpha, \beta, \gamma, x_{i}, \tau^{2}) = \\
= \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi\tau^{2}}} \cdot exp\Big\{-\frac{1}{2\tau^{2}} ( y_{i} - \mu_{i})^{2} \Big\} = \\
= (2\pi\tau^{2})^{-\frac{n}{2}} \cdot exp\Big\{ -\frac{1}{2\tau^{2}} \sum_{i=1}^{n}\Big( y_{i} - (\alpha - \beta\gamma^{x_{i}})\Big)^{2} \Big\}
$$

$$~$$

# 1.c) Joint prior distribution of the parameters

$$~$$

We need to write the joint prior distribution at stake, considering the $\alpha, \beta, \gamma \,\, and \,\, \tau^{2}$. It is important to denote the fact that the prior is written as a multiplication of whole hyperparameters interested in our analysis:

$$
\pi(\alpha, \beta, \gamma, \tau^{2}) = f_{Normal(0, \sigma_{\alpha}^{2})}(\alpha) \cdot f_{Normal(0, \sigma_{\beta}^{2})}(\beta) \cdot f_{Unif(0, 1)}(\gamma) \cdot f_{InvGamma(a, b)}(\tau^{2}) \propto \\
\propto exp\Big\{ -\frac{\alpha^{2}}{2\sigma_{\alpha}^{2}} \Big\} \cdot exp\Big\{ -\frac{\beta^{2}}{2\sigma_{\beta}^{2}} \Big\} \cdot \frac{exp\Big\{ -\frac{b}{\tau^{2}} \Big\}}{\tau^{2(a+1)}} \cdot \mathbf{I}_{[0, 1]}(\gamma)
$$
<!--you need to put the indicator functions in the latex formulaa-->

In this step, I choose suitable hyperparameters for this statistical model, in order to fit well with our main features recognized before.

$$~$$

### **Giving a suitable choice for tau**

$$~$$

$\tau^2$ is the variance of our observations, we consider a combination of the prior hyper parameters, where $\tau^2 \sim InvGamma(a, b)$, through simulations we obtained this behaviours:


```{r echo=FALSE, fig.align="center"}
library(invgamma)
a <- c(0.5, 1.5, 3.5, 5.5, 7.5, 9.5, 15, 21)
b <- c(0.5, 1.7, 3.5, 5.5, 7.5, 9.5, 15, 20)

# check the particular good combination manually and then dinamically
curve(invgamma::dinvgamma(x, shape = a[8], rate = b[2]), col = "red", lwd = 3, ylim = c(0, 25), xlim = c(0, 0.5), xlab = expression(tau^2), ylab = expression(pi(tau^2)), main = "The suitable choiche of tau")
for(i in 1:length(a)){
  for(j in 1:length(b)){
    curve(invgamma::dinvgamma(x, shape = a[i], rate = b[j]), col = randomcoloR::randomColor(), add = TRUE)
  }
}

abline(v = var(y_obs), lwd = 2)
grid()
```

As we shown above, the good prior parameters a and b for the hyperparameter $\tau^2$ are a = 21 and b = 1.7, because they give a small uncertainty than others.

$$~$$

### **Giving a suitable choice for mu**

$$~$$

After deciding the $\tau^{2}$ hyperparameter is important to decide which hyper-values attribute for the mean of the statistical model, is important to remind that the mean is described as $\mu_{i} = f(x_{i}) = \alpha - \beta\gamma^{x_{i}}$. Given the fact that $\mu_{i}$ is described as a non-linear model, we need to choose suitable hyperparameters for $\alpha \, \text{and} \, \beta$, so the first hyperparameter needs to choose the variance denoted as $\sigma_{\alpha}^{2}$ by simulations:

```{r echo=FALSE, fig.align="center", warning=FALSE}
# Fitting the model using the function fit
fitted <- lm(Length = x_i, data = dugongdat)

sig_alph <- c(6, 1, 7, 8, 10, 20)
curve(dnorm(x, mean = 0, sd = sqrt(sig_alph[1])), lwd = 2, xlab = expression(alpha), ylab = "Prior alpha", ylim = c(0, 0.5), xlim=c(-6,6), col = "red")

for(i in 2:length(sig_alph)) curve(dnorm(x, mean = 0, sd = sqrt(sig_alph[i])), add = T, col = randomcoloR::randomColor())

# the proper value
abline(v = coefficients(fitted)[1], lwd = 2, lty = 2, col = "purple")
grid()
```

In this case, we want to assign for $\sigma_{\alpha}^{2}$ the value of 6, that shows a lower uncertainty than others, the higher value of probability of the red curve is near to the optimal dashed purple line found. This contains the intercept of the regression.

Instead, for $\sigma_{\beta}^{2}$ we find by simulations:

```{r echo=FALSE, fig.align="center", warning=FALSE}
sig_bet <- c(0.05, 0.5, 0.7, 0.9, 1.1, 2)
curve(dnorm(x, mean = 0, sd = sqrt(sig_bet[1])), lwd = 2, xlab = expression(alpha), ylab = "Prior alpha", ylim = c(0, 2), xlim=c(-1,1), col = "red")

for(i in 2:length(sig_bet)) curve(dnorm(x, mean = 0, sd = sqrt(sig_bet[i])), add = T, col = randomcoloR::randomColor())

# the proper value
abline(v = coefficients(fitted)[2], lwd = 2, lty = 2, col = "purple")
grid()
```

In this case, the value of sigma beta is 0.05, as we can seen above this gives the best fitting.

Instead, for $\gamma$ we have the same prior parameters of the uniform distribution, the range is within 0 and 1.

$$~$$

# 1.d) Functional form of all full-conditionals 

$$~$$

To derive the full-conditionals is important to specify, how much is needed in our case. The number of full-conditionals required in this case is 4, considering our hyperparameters: $\alpha, \beta, \gamma, \tau^{2}$.

Before starting, is important to denote that $\mu_{i}$ is given by $\alpha, \beta, \gamma, x_{i}$ and $x_{i} = {x_{1}, ... , x_{n}}$. So, let's start with $\tau^{2}$:

$$
\pi(\tau^{2}|\alpha, \beta, \gamma, y_{obs}) \propto \pi(y_{obs}|\alpha, \beta, \gamma, x_{i}, \tau^{2}) \pi(\tau^{2}) \propto \\
\propto \pi(y_{obs}|\mu_{i}, \tau^{2}) \pi(\tau^{2}) \propto \\
\propto (2\pi\tau^{2})^{-\frac{n}{2}} \cdot exp\Big\{ -\frac{1}{2\tau^{2}} \sum_{i=1}^{n}\Big( y_{i} - (\alpha - \beta\gamma^{x_{i}})\Big)^{2} \Big\} \cdot (\tau^{2})^{-(a+1)} exp\Big\{ -\frac{b}{\tau^{2}} \Big\} \propto \\
\propto (\tau^{2})^{-\frac{n}{2}} \cdot exp\Big\{ -\frac{1}{2\tau^{2}} \sum_{i=1}^{n}\Big( y_{i} - (\alpha - \beta\gamma^{x_{i}})\Big)^{2} -\frac{b}{\tau^{2}} \Big\} \cdot (\tau^{2})^{-(a+1)} \propto \\
\propto (\tau^{2})^{-(\frac{n}{2}+a+1)} \cdot exp\Big\{ -\frac{\sum_{i=1}^{n}( y_{i} - (\alpha - \beta\gamma^{x_{i}}))^{2}+b}{2\tau^{2}} \Big\}
$$

```{r include=FALSE}
# full-conditional with known distro of tau
fullcond_post_tau <- function(y_obs, a, b, alpha, beta, gamma, x_ages){
  shape_post <- a+length(y_obs)/2
  
  rate_post <- 0
  for(i in 1:length(y_obs)) rate_post <- rate_post + (y_obs[i]-(alpha-beta*(gamma^x_ages[i])))^2
  rate_post <- b + rate_post/2
  
  draw <- invgamma::rinvgamma(n = 1, shape = shape_post, rate = rate_post)
  return(draw)
}
```


Instead, the other full-conditionals are, for $\alpha$:

$$
\pi(\alpha|\tau^{2}, \beta, \gamma, y_{obs}) \propto \pi(y_{obs}|\alpha, \beta, \gamma, x_{i}, \tau^{2}) \pi(\alpha) \propto \\
\propto \pi(y_{obs}|\mu_{i}, \tau^{2}) \pi(\alpha) \propto \\
\propto (2\pi\tau^{2})^{-\frac{n}{2}} \cdot exp\Big\{ -\frac{1}{2\tau^{2}} \sum_{i=1}^{n}\Big( y_{i} - (\alpha - \beta\gamma^{x_{i}})\Big)^{2} \Big\} \cdot exp\Big\{ -\frac{(\alpha)^2}{2\sigma_{\alpha}^{2}} \Big\} \propto \\
\propto exp\Big\{ -\frac{1}{2\tau^{2}} \sum_{i=1}^{n}\Big( y_{i} - (\alpha - \beta\gamma^{x_{i}})\Big)^{2} -\frac{(\alpha)^2}{2\sigma_{\alpha}^{2}} \Big\} \propto \\
\propto exp\Big\{ -\frac{1}{2\tau^{2}} \sum_{i=1}^{n} \Big( y_{i}^{2} + \alpha^{2} + \beta^{2}\gamma^{2x_{i}} - 2y_{i}\alpha + 2y_{i}\beta\gamma^{x_{i}} -  2\alpha\beta\gamma^{x_{i}} \Big) -\frac{(\alpha)^2}{2\sigma_{\alpha}^{2}} \Big\} \propto \\
\propto exp\Big\{ -\frac{1}{2} \Big( \frac{n}{\tau^{2}} + \frac{1}{\sigma_{\alpha}^{2}}\Big)\alpha^{2} + \Big( \frac{\sum_{i=1}^{n} y_{i} + \beta\sum_{i=1}^{n}\gamma^{x_{i}}}{\tau^{2}} \Big)\alpha \Big\}
$$

```{r include=FALSE}
# unknown distro of alpha
fullcond_post_alpha <- function(y_obs, alpha, beta, gamma, x_ages, tau, sigma_2_alpha){
  # update the theta hyperparameter(alpha or beta depends the formula is the same! changes sligthly)
  mu_post <- (sigma_2_alpha / (length(y_obs)*sigma_2_alpha + tau)) * sum(y_obs+beta*gamma^x_ages)
  
  sd_post <- (sigma_2_alpha*tau)/(length(y_obs)*sigma_2_alpha + tau)
  
  draw <- rnorm(1, mean = mu_post, sd = sqrt(sd_post))
  if(draw < 1) draw <- 1
  return(draw)
}
```

for $\beta$:

$$
\pi(\beta|\tau^{2}, \alpha, \gamma, y_{obs}) \propto \pi(y_{obs}|\alpha, \beta, \gamma, x_{i}, \tau^{2}) \pi(\beta) \propto \\
\propto \pi(y_{obs}|\mu_{i}, \tau^{2}) \pi(\beta) \propto \\
\propto (2\pi\tau^{2})^{-\frac{n}{2}} \cdot exp\Big\{ -\frac{1}{2\tau^{2}} \sum_{i=1}^{n}\Big( y_{i} - (\alpha - \beta\gamma^{x_{i}})\Big)^{2} \Big\} \cdot exp\Big\{ -\frac{(\beta)^2}{2\sigma_{\beta}^{2}} \Big\} \propto \\
\propto exp\Big\{ -\frac{1}{2\tau^{2}} \sum_{i=1}^{n}\Big( y_{i} - (\alpha - \beta\gamma^{x_{i}})\Big)^{2} -\frac{(\beta)^2}{2\sigma_{\beta}^{2}} \Big\} \propto \\
\propto exp\Big\{ -\frac{1}{2\tau^{2}} \sum_{i=1}^{n} \Big( y_{i}^{2} + \alpha^{2} + \beta^{2}\gamma^{2x_{i}} - 2y_{i}\alpha + 2y_{i}\beta\gamma^{x_{i}} -  2\alpha\beta\gamma^{x_{i}} \Big) -\frac{(\beta)^2}{2\sigma_{\beta}^{2}} \Big\} \propto \\
\propto exp\Big\{ -\frac{1}{2} \Big( \frac{\sum_{i=1}^{n} \gamma^{2x_{i}}}{\tau^{2}} + \frac{1}{\sigma_{\beta}^{2}}\Big)\beta^{2} + \Big( \frac{ \alpha\sum_{i=1}^{n}\gamma^{x_{i}} - \sum_{i=1}^{n}y_{i}\gamma^{x_{i}} }{\tau^{2}} \Big)\beta \Big\}
$$

```{r include=FALSE}
# unknown distro of beta
fullcond_post_beta <- function(y_obs, alpha, beta, gamma, x_ages, tau, sigma_2_beta){
  # update the theta hyperparameter(alpha or beta depends the formula is the same! changes sligthly)
  mu_post <- (sigma_2_beta/(sigma_2_beta*sum(gamma^(2*x_ages)+tau)))*sum((alpha-y_obs)*gamma^x_ages)
  
  sd_post <- (sigma_2_beta*tau)/(sigma_2_beta*sum(gamma^(2*x_ages)+tau))
  
  draw <- rnorm(1, mean = mu_post, sd = sqrt(sd_post))
  if(draw < 1) draw <- 1
  return(draw)
}
```

for $\gamma$:

$$
\pi(\gamma|\tau^{2}, \alpha, \beta, y_{obs}) \propto \pi(y_{obs}|\alpha, \beta, \gamma, x_{i}, \tau^{2}) \pi(\gamma) \propto \\
\propto \pi(y_{obs}|\mu_{i}, \tau^{2}) \pi(\gamma) \propto \\
\propto (2\pi\tau^{2})^{-\frac{n}{2}} \cdot exp\Big\{ -\frac{1}{2\tau^{2}} \sum_{i=1}^{n}\Big( y_{i} - (\alpha - \beta\gamma^{x_{i}})\Big)^{2} \Big\} \cdot \mathbb{I_{[0,1]}(\gamma)} \propto \\
\propto exp\Big\{ -\frac{1}{2\tau^{2}} \sum_{i=1}^{n}\Big( y_{i} - (\alpha - \beta\gamma^{x_{i}})\Big)^{2} \Big\} \cdot \mathbb{I_{[0,1]}(\gamma)} \propto \\
\propto exp\Big\{ -\frac{1}{2\tau^{2}} \sum_{i=1}^{n} \Big( \beta^{2}\gamma^{2x_{i}} - 2\alpha\beta\gamma^{x_{i}} + 2y_{i}\beta\gamma^{x_{i}} \Big) \Big\}
$$

```{r include=FALSE}
# unknown distro of gamma
notknown_fullcond_post_gamma <- function(y_obs, alpha, beta, gamma, x_ages, tau){
  # get the summatory of each yobs with each ui
  sum_for_postgamma <- 0
  for(i in 1:length(y_obs)) sum_for_postgamma <- sum_for_postgamma + ( (beta^2)*gamma^(2*x_ages[i]) - 2*alpha*beta*gamma^(x_ages[i]) + 2*y_obs[i]*beta*gamma^(x_ages[i]))
  
  # update the theta hyperparameter gamma
  gamma_post <- ((-1/(2*tau))*sum_for_postgamma)*1
  return(gamma_post)
}
```

As we can seen above, there are known and unknown distributions.

$$~$$

# 1.e) Which distribution can you recognize within standard parametric families so that direct simulation from full conditional can be easily implemented

$$~$$

As we can seen, in the previous section we derived the full-conditionals of the four hyper-parameters $\alpha, \beta, \gamma, \tau^{2}$. 

We recognized that the distribution of $\tau^{2} \sim InvGamma\Big(a+\frac{n}{2}, \,\, b + \frac{\sum_{i=1}^{n}(y_{i}-(\alpha-\beta\gamma^{x_{i}}))^2}{2} \Big)$ and also the $\alpha \sim N\Big( \hat{\mu} = \frac{\frac{\sum_{i=1}^{n} y_{i} + \beta\sum_{i=1}^{n}\gamma^{x_{i}}}{\tau^{2}}}{\frac{n}{\tau^{2}} + \frac{1}{\sigma_{\alpha}^{2}}} , \,\, \hat{\sigma^{2}} = \frac{1}{\frac{n}{\tau^{2}} + \frac{1}{\sigma_{\alpha}^{2}}} \Big) \, and \, \beta \sim N\Big( \hat{\mu} = \frac{\frac{ \alpha\sum_{i=1}^{n}\gamma^{x_{i}} - \sum_{i=1}^{n}y_{i}\gamma^{x_{i}} }{\tau^{2}}}{\frac{\sum_{i=1}^{n} \gamma^{2x_{i}}}{\tau^{2}} + \frac{1}{\sigma_{\beta}^{2}}} , \,\, \hat{\sigma^{2}} = \frac{1}{\frac{\sum_{i=1}^{n} \gamma^{2x_{i}}}{\tau^{2}} + \frac{1}{\sigma_{\beta}^{2}}}\Big)$ are known distributions (by the usual-fact that we can seen in the slide 95) only the $\gamma$ hyperparameter is the only one which we don't know the proper distribution.

We can continue to formalize well the parameters that describing $\alpha \, \text{and} \, \beta$, for $\alpha$ we have:

$$
\alpha \sim N\Big( \hat{\mu} = \frac{\frac{\sum_{i=1}^{n} y_{i} + \beta\sum_{i=1}^{n}\gamma^{x_{i}}}{\tau^{2}}}{\frac{n}{\tau^{2}} + \frac{1}{\sigma_{\alpha}^{2}}} , \,\, \hat{\sigma^{2}} = \frac{1}{\frac{n}{\tau^{2}} + \frac{1}{\sigma_{\alpha}^{2}}} \Big) = N\Big(\hat{\mu} =  \frac{\sigma_{\alpha}^{2}}{n\sigma_{\alpha}^{2}+\tau^{2}}\sum_{i=1}^{n}y_{i}+\beta\gamma^{x_{i}}, \,\, \hat{\sigma^{2}} = \frac{\sigma_{\alpha}^{2}\tau^{2}}{n\sigma_{\alpha}^{2}+\tau^{2}} \Big)
$$

and for $\beta$ we have:

$$
\beta \sim N\Big( \hat{\mu} = \frac{\frac{ \alpha\sum_{i=1}^{n}\gamma^{x_{i}} - \sum_{i=1}^{n}y_{i}\gamma^{x_{i}} }{\tau^{2}}}{\frac{\sum_{i=1}^{n} \gamma^{2x_{i}}}{\tau^{2}} + \frac{1}{\sigma_{\beta}^{2}}} , \,\, \hat{\sigma^{2}} = \frac{1}{\frac{\sum_{i=1}^{n} \gamma^{2x_{i}}}{\tau^{2}} + \frac{1}{\sigma_{\beta}^{2}}}\Big) = N\Big( \hat{\mu} = \frac{\sigma_{\beta}^{2}}{\sigma_{\beta}^{2}\sum_{i=1}^{n}\gamma^{2x_{i}}+\tau^{2}}\sum_{i=1}^{n}\gamma^{x_{i}}(\alpha-y_{i}), \,\, \hat{\sigma^{2}} = \frac{\sigma_{\beta}^{2}\tau^{2}}{\sigma_{\beta}^{2}\sum_{i=1}^{n}\gamma^{2x_{i}}+\tau^{2}} \Big)
$$

So, we can directly simulate the full-conditional for $\alpha, \beta, \tau^{2}$.

But, for $\gamma$ we must do a slightly different changment.

$$~$$

# 1.f) Using a suitable Metropolis-within-Gibbs algorithm simulate a Markov chain (T = 10000) to approximate the posterior distribution for the above model

$$~$$

In this step, we use the Hybrid-Kernels technique composed in this case as Metropolis-within-Gibbs. For this reason, we simply have Gibbs samples through the known distribution of $\alpha, \beta, \tau^{2}$ and after that, we infer on the $\gamma$ hyperparameter where the distribution is not known. We can handle the *unormalized* full conditional with the Metropolis Hastings within the Gibbs samplig.

The ***Gibbs Sampling*** is an algorithm for getting draws from a posterior distribution when the conditional posteriors are known, while we can use ***Metropolis-Hastings (MH)*** to sample from a non-conjugate conditional posterior within each Gibbs iteration.

the conditional posterior of $\alpha, \beta, \tau^{2}$ is conjugate. Inside each Gibbs iteration we can use a standard function to sample from their proper distributions; we don't need other second sampling here. Instead, for the $\gamma$ hyperparameter we need to have a second sampler that is the ***MH***.

### Metropolis-Hastings Algorithm

The goal is to sample from each conditional posterior. As we can seen below, the MH sampler works in this way (for this explanation we define a general hyperparameter $\theta$):

1. Define $\pi$ as to be the target density and produce a started candidate of $\theta^{(0)}$ in order to start the sampling. 

2. Let $p_{x}(y)$ to be a ***proposal distribution***, the value of x depends on the present value that has in the chain.

3. Draw a candidate $Y_{t+1} \sim p_{x}(y)$ and denotes that y is the realized candidate $Y_{t+1} = y$

4. Decide whether or not the candidate is accepted as the next state of this chain at time t+1, if not set the next state equal to the current state x of the chain:

$$
X_{t+1} = 
\begin{cases}
  \text{y with probability} \,\, \alpha(x, y) \\
  \text{x with probability} \,\, 1-\alpha(x, y)
\end{cases}
$$

where $\alpha(x,y)$ = $min\Big\{\frac{\pi(y)p_{y}(x)}{\pi(x)p_{x}(y)},1 \Big\}$

So, proposals that yield a higher conditional posterior evaluation are always accepted. However, proposals with lower density evaluations are only sometimes accepted. Over many iterations, draws from the posterior’s high density areas are accepted and the sequence of accepted proposals “climbs” to the high density area. Once the sequence arrives in this high-density area, it tends to remain there, so:

```{r}
# MH algorithm to sample from the notknown-full-cond of gamma
notknown_fullcond_post_gamma_MH <- function(y_obs, alpha, beta, currentstate, x_ages, tau, a, greekpi, mhtrials){ # greekpi is the target distro
  gamma_t <- currentstate # current state of theta in the chain
  
  for(i in 1:mhtrials){
    # draw a candidate for the next/future state of the chain 
    gamma_prop <- runif(1, min=0, max=1)
    
    # acceptance/rejection auxiliary draw
    r_num <- greekpi(y_obs = y_obs, alpha = alpha, beta = beta, gamma = gamma_prop, x_ages = x_ages, tau = tau)
    r_den <- greekpi(y_obs = y_obs, alpha = alpha, beta = beta, gamma = gamma_t, x_ages = x_ages, tau = tau)
    
    r <- exp(r_num-r_den)
    rmin <- min(r, 1)
    
    # acc/rj method
    omega = runif(1,min=0,max=1)
    ACCEPT=(omega < rmin)
    gamma_t <- ifelse(ACCEPT,gamma_prop, gamma_t)
  }
  
  return(gamma_t)
}

# set the number of iterations T
niter <- 10000

# set the prior parameters of tau and the the prior parameters for sigma(alpha and beta)
a <- 21 
b <- 1.7
sigma_2_a <- 6
sigma_2_b <- 0.05

# define the variables at every iteration t
gibbs_samples <- data.frame(tau = rep(NA, niter), alpha = rep(NA, niter), beta = rep(NA, niter), gamma = rep(NA, niter))

# initialize the state t = 0
gibbs_samples[1, 1:4] <- c(0.03, 0, 0, 0) # tau is greater than zero, because is on the denominator 

# save for each iteration the mean of the parameters ( useful for the point 1.h )
mean_at_time_t <- list(mean_tau = rep(NA, niter), mean_alpha = rep(NA, niter), mean_beta = rep(NA, niter), mean_gamma = rep(NA, niter))
# sampling with the Hybrid-Kernels method
for(t in 1:niter){
  # sample from posterior full-conditional of tau
  gibbs_samples[t+1, "tau"] <- fullcond_post_tau(y_obs, a, b, alpha = gibbs_samples[t, "alpha"], beta = gibbs_samples[t, "beta"], gamma = gibbs_samples[t, "gamma"], x_i)
  
  # define the random-walk MH for the other parameters alpha, beta and gamma
  gibbs_samples[t+1, "alpha"] <- fullcond_post_alpha(y_obs = y_obs, alpha = gibbs_samples[t, "alpha"], beta = gibbs_samples[t, "beta"], gamma = gibbs_samples[t, "gamma"], x_ages = x_i, tau = gibbs_samples[t, "tau"], sigma_2_alpha = sigma_2_a)

  gibbs_samples[t+1, "beta"] <- fullcond_post_beta(y_obs = y_obs, alpha = gibbs_samples[t, "alpha"], beta = gibbs_samples[t, "beta"], gamma = gibbs_samples[t, "gamma"], x_ages = x_i, tau = gibbs_samples[t, "tau"], sigma_2_beta = sigma_2_b)
  

  gibbs_samples[t+1, "gamma"] <- notknown_fullcond_post_gamma_MH(y_obs, alpha = gibbs_samples[t, "alpha"], beta = gibbs_samples[t, "beta"], currentstate = gibbs_samples[t, "gamma"], x_ages = x_i, tau = gibbs_samples[t, "tau"], a = 1, greekpi = notknown_fullcond_post_gamma, mhtrials = 5)
  
  # save for each iteration the mean of the parameters ( useful for the point 1.h )
  mean_at_time_t$mean_alpha[t] <- mean(gibbs_samples[1:t, "alpha"])
  mean_at_time_t$mean_beta[t] <- mean(gibbs_samples[1:t, "beta"])
  mean_at_time_t$mean_gamma[t] <- mean(gibbs_samples[1:t, "gamma"])
  mean_at_time_t$mean_tau[t] <- mean(gibbs_samples[1:t, "tau"])
}
```

$$~$$

After running the algorithm, let's go to see the traceplots of these hyperparameters

$$~$$

# 1.g) Show the 4 univariate trace-plots of the simulations of each parameter

$$~$$

In this section we show the behaviours of these 4 hyperparameters in order to see interesting results:

```{r echo=FALSE, fig.align="center"}
plot(gibbs_samples[, "tau"],type="l",xlab="t",ylab=expression(tau^2), col = randomcoloR::randomColor(), ylim = c(0, 0.35))
title(main="Trace plot of the MC")
grid()
plot(gibbs_samples[, "alpha"],type="l",xlab="t",ylab=expression(alpha), col = randomcoloR::randomColor(), ylim = c(2.0, 3.2))
title(main="Trace plot of the MC")
grid()
plot(gibbs_samples[, "beta"],type="l",xlab="t",ylab=expression(beta), col = randomcoloR::randomColor(), ylim = c(1, 1.3))
title(main="Trace plot of the MC")
grid()
plot(gibbs_samples[, "gamma"],type="l",xlab="t",ylab=expression(gamma), col = randomcoloR::randomColor())
title(main="Trace plot of the MC")
grid()
```

$$~$$

The plots obtained are pretty interesting! Is important to denote that $\tau^{2}$ is fixed by a number larger than 0, to avoid the problem of infinity value. If we change also the values of a, b, $\sigma_{a}^{2}$ and $\sigma_{b}^{2}$ the behaviour will be similar.

$$~$$

# 1.h) Evaluate graphically the behaviour of the empirical averages ˆIt with growing t = 1, ..., T

$$~$$

In this section we want to see the empirical average of $\mathbf{\hat{I}}_{t}$ increasing the value of $t \, = \, 1,...,T$. Before starting, we want to write the formula of $\mathbf{\hat{I}}_{t}$ that is:

$$~$$

$$
\mathbf{I} \approx \mathbf{\hat{I}}_{t} = \frac{1}{T} \sum_{i=1}^{T} h(\theta^{(i)})
$$

$$~$$

is important to write $\approx$ because this leverages the SLLN theorem and we want also to confirm this assumption! So, let's see the results:

$$~$$

```{r echo=FALSE, fig.align="center"}
plot(mean_at_time_t$mean_tau,type="l",xlab="t",ylab=expression(tau^2), lwd=2, col = "orchid2")
title(main="Empirical Average through T iterations")
grid()
plot(mean_at_time_t$mean_alpha,type="l",xlab="t",ylab=expression(alpha), lwd=2, col = "orchid2")
title(main="Empirical Average through T iterations")
grid()
plot(mean_at_time_t$mean_beta,type="l",xlab="t",ylab=expression(beta), lwd=2, col = "orchid2")
title(main="Empirical Average through T iterations")
grid()
plot(mean_at_time_t$mean_gamma,type="l",xlab="t",ylab=expression(gamma), lwd=2, col = "orchid2")
title(main="Empirical Average through T iterations")
grid()
```

$$~$$

As we can seen above, the mean of each hyperparameters follows approximately the behaviour of the markov chain, this is amazing!

$$~$$

# 1.i) Provide estimates for each parameter together with the approximation error and explain how you have evaluated such error

$$~$$

Here, there are the averages of the simulations of each parameter analyzed:

$$~$$

```{r echo=FALSE}
cat(paste("the average of tau at t iterations is: ", round(tail(mean_at_time_t$mean_tau, n=1), 4), "\n", sep = "") )
cat(paste("the average of alpha at t iterations is: ", round(tail(mean_at_time_t$mean_alpha, n=1), 4), "\n", sep = "") )
cat(paste("the average of beta at t iterations is: ", round(tail(mean_at_time_t$mean_beta, n=1), 4), "\n", sep = "") )
cat(paste("the average of gamma at t iterations is: ", round(tail(mean_at_time_t$mean_gamma, n=1), 4), "\n", sep = "") )
```

$$~$$

We consider a particular tool to estimate the approximation error in order to taking into account positive and possible autocorrelations between the samples that we sampled before. 

We can also visualize the different auto-correlation functions of each hyperparameter:

```{r echo=FALSE, fig.align="center"}
acf(gibbs_samples[, "tau"], main="ACF at different lags of the simulations", xlab="lag", ylab=expression(rho[tau]))
grid()
acf(gibbs_samples[, "alpha"], main="ACF at different lags of the simulations", xlab="lag", ylab=expression(rho[alpha]))
grid()
acf(gibbs_samples[, "beta"], main="ACF at different lags of the simulations", xlab="lag", ylab=expression(rho[beta]))
grid()
acf(gibbs_samples[, "gamma"], main="ACF at different lags of the simulations", xlab="lag", ylab=expression(rho[gamma]))
grid()
```

The ACFs follow good behaviours, because we need to have sample independent each other during new iterations. Here, we see that the correlation that going further from the first lag, this is strictly decreasing and this is a good point!

So, in this case we consider the Monte Carlo Standard Error (MCSE) that is an estimate of the inaccuracy of MC samples for the expectation of posterior samples.

MCSE is the standard deviation around the posterior mean of these samples associated to their uncertainty during the use of the MCMC or MC algorithms. 

$$~$$

```{r echo=FALSE}
cat(paste("the MCSE of tau at t iterations is: ", round(LaplacesDemon::MCSE(gibbs_samples[,"tau"]), 3), "\n", sep=""))
cat(paste("the MCSE of alpha at t iterations is: ", round(LaplacesDemon::MCSE(gibbs_samples[,"alpha"]), 3), "\n", sep=""))
cat(paste("the MCSE of beta at t iterations is: ", round(LaplacesDemon::MCSE(gibbs_samples[,"beta"]), 3), "\n", sep=""))
cat(paste("the MCSE of gamma at t iterations is: ", round(LaplacesDemon::MCSE(gibbs_samples[,"gamma"]), 3), "\n", sep=""))
```

$$~$$

these values are pretty interesting because our estimation errors are lower, so we can have maybe the good uncertainty that we are in the correct region where sampling the parameters.

$$~$$

# 1.l) Which parameter has the largest posterior uncertainty? How did you measure it?

$$~$$

In order to measure which is the posterior parameter with the largest posterior uncertainty, we use the formula of the variance in the case of MCMC and not in the case of MC, because the formula is sligthly different and because with the variance of MC we consider the samples as independent. 

We emphasized the fact that the $\mathbf{E}[\mathbf{\hat{I}}_{t}] = \mathbf{E}_{\pi}[h(X)]$ but for the variance we have:

$$
\mathbf{V}[\hat{I}_{t}] = Var\Big[ \frac{1}{T} \sum_{i=1}^{T} h(X_{t}) \Big] = \frac{1}{T^{2}}\Big[\sum_{i=1}^{T}Var\Big[h(X_{t})\Big] + \sum_{i=1}^{T} \sum_{s \neq t} Cov\Big[h(X_{t}), h(X_{s})\Big]\Big]
$$

But, considering that the MC is stationary the covariance is the same whenever the lag is the same, we achieve to say that the variance is:

$$
\mathbf{V}[\hat{I}_{t}] = \frac{Var_{\pi}\Big[h(X_{1})\Big]}{T}\Big[ 1 + 2\sum_{k=1}^{T-1}(\frac{T-k}{T})\rho_{k}\Big]
$$

Considering the fact that:

$$
Var[\sqrt{T}\hat{I}_{t}] = T\sigma_{\hat{I}_{t}}^{2} \rightarrow \sigma^{2} \Big( 1 + 2\sum_{k=1}^{\infty} \rho_{k} \Big)
$$

The factor inside the parentheses is greater than 1. This is an inefficiency factor as long as $\rho_{k}$ prevails, so is used to compute the effective sample size ESS is:

$$
T_{eff} = \frac{T}{1+2\sum_{k=1}^{\infty}\rho_{k}}
$$

This advices to use the correct number of samples in order to achieve the stationary state, in fact we used it in the variance formula in this way:

$$
\mathbf{V}[\hat{I}_{t}] = \frac{Var_{\pi}[h(X_{1})]}{T_{eff}} = \Big( 1 + 2 \sum_{k=1}^{\infty} \rho_{k}\Big)\frac{\sigma^{2}}{T}
$$

Counting this last consideration, we are looking for each variance of each hyper parameter analyzed:

$$~$$

```{r echo=FALSE}
cat(paste("the variance of tau at t iterations is: ", var(gibbs_samples[,"tau"])/LaplacesDemon::ESS(gibbs_samples[,"tau"]), "\n", sep=""))
cat(paste("the variance of alpha at t iterations is: ", var(gibbs_samples[,"alpha"])/LaplacesDemon::ESS(gibbs_samples[,"alpha"]), "\n", sep=""))
cat(paste("the variance of beta at t iterations is: ", var(gibbs_samples[,"beta"])/LaplacesDemon::ESS(gibbs_samples[,"beta"]), "\n", sep=""))
cat(paste("the variance of gamma at t iterations is: ", var(gibbs_samples[,"gamma"])/LaplacesDemon::ESS(gibbs_samples[,"gamma"]), "\n", sep=""))
```

$$~$$

the hyperparameter with the largest posterior uncertainty is $\alpha$

$$~$$

# 1.m) Which couple of parameters has the largest correlation (in absolute value)?

$$~$$
 
Representing the correlations of the values using the heatmap, we see:

$$~$$

```{r echo=FALSE, fig.align="center"}
corrplot::corrplot(abs(cor(gibbs_samples)), method = "color", addCoef.col="grey", order = "AOE")
```

$$~$$

The largest value of correlation is between $\gamma$ and $\alpha$.

$$~$$

# 1.n) Use the Markov chain to approximate the posterior predictive distribution of the length of a dugong with age of 20 years

$$~$$

Basically, when we want to fill in missing data, discover new data and something else  is important to do predictions about the future. In our case using MCMC is trivial to find the predictive distributions.

Suppose to have a model $\pi(y_{pred}|\theta)$ and a fully known prior distribution. As we known, the Monte Carlo produces the posterior predictive distribution of a new (future) quantity as:

$$
\pi(y_{pred}) = \int \pi(y_{pred}|\theta)\pi(\theta)d\theta
$$

considering $y_{pred}$ within the model and considering it as an unknown quantity. This option could be included within the MCMC methods with a posterior distribution $\pi(\theta|y)$

To approximate the posterior predictive distribution using MCMC we simply do this steps:

1. Removes the burn-in period where the values are unstabilized
2. Generates different means with the age of dugong equal to 20
3. Generates the different observations
4. Approximates the posterior predictive distribution and sees the Monte Carlo standard deviation as the best approximate standard deviation error

$$~$$

```{r echo=FALSE}
# Removing the burn-in period of the first 500 samples
burn_in <- 501

tau_t <- gibbs_samples[burn_in:niter, "tau"]
alpha_t <- gibbs_samples[burn_in:niter, "alpha"]
beta_t <- gibbs_samples[burn_in:niter, "beta"]
gamma_t <- gibbs_samples[burn_in:niter, "gamma"]

# Generating the means with the dugong age equal to 20
mu_posteriorpredictives <- alpha_t - beta_t*(gamma_t^20)

# Generating the observations
y_preds <- rnorm(n = niter - burn_in, mean = mu_posteriorpredictives, sd = tau_t)

# Showing the expectation of the observations
cat(paste("The approximation of the posterior predictive distribution is: ", round(mean(y_preds), 3), "\n", sep = ""))
cat(paste("The MCSE of the posterior predictive distribution is: ", round(LaplacesDemon::MCSE(y_preds), 3), "\n", sep = ""))
```

```{r warning=FALSE, echo=FALSE, fig.align="center"}
hist(y_preds, col = randomcoloR::randomColor(), main = "Predictive observations based of having 20 years old Dugongs", xlim = c(2.2, 3.0), breaks=100)
grid()
rug(y_preds)
```

$$~$$

# 1.o) Provide the prediction of a different dugong with age 30

$$~$$

Considering the same steps followed in the previous section, we have:

$$~$$

```{r echo=FALSE}
# Removing the burn-in period of the first 500 samples
burn_in <- 501

tau_t <- gibbs_samples[burn_in:niter, "tau"]
alpha_t <- gibbs_samples[burn_in:niter, "alpha"]
beta_t <- gibbs_samples[burn_in:niter, "beta"]
gamma_t <- gibbs_samples[burn_in:niter, "gamma"]

# Generating the means with the dugong age equal to 30
mu_posteriorpredictives <- alpha_t - beta_t*(gamma_t^30)

# Generating the observations
y_preds <- rnorm(n = niter - burn_in, mean = mu_posteriorpredictives, sd = tau_t)

# Showing the expectation of the observations
cat(paste("The approximation of the posterior predictive distribution is: ", round(mean(y_preds), 3), "\n", sep = ""))
cat(paste("The MCSE of the posterior predictive distribution is: ", round(LaplacesDemon::MCSE(y_preds), 3), "\n", sep = ""))
```

```{r warning=FALSE, echo=FALSE, fig.align="center"}
hist(y_preds, col = randomcoloR::randomColor(), main = "Predictive observations based of having 30 years old Dugongs", xlim = c(2.2, 3.0), breaks=100)
grid()
rug(y_preds)
```

$$~$$

# 1.p) Which prediction is less precise?

$$~$$

The second prediction gives less precision than the previous. Due, to the approximation errors (MCSE) of the corresponding cases.

$$~$$

# Test Part 2

$$~$$

